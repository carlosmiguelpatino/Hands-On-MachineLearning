{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "\n",
    "The idea behind a Support Vector Machine classifier is to establish a boundary that has the widest separation between the classes. Since the objective of this algorithm is to establish the widest \"street\" between two classes, instances that are outside the street are not going to afect the location of the boundary. Instead, the boundary between the classes is going to be fully determined by the instaces that are in the limits of the \"street\" between the two classes.\n",
    "\n",
    "It is important to keep in mind that Support Vector Machines are sensitive to feature scaling.\n",
    "\n",
    "## Soft Margin Classification\n",
    "\n",
    "The condition of having the instances of each class in separate sides of the street is called *hard margin classification*. The problem with establishing a strict condition like that one is that the algorithm would only work properly in linearly separable classes. If only an instance of a class is located near the other class in the feature space, the algorithm will perform poorly. In other words, the hard margin classification is sensitive to outliers.\n",
    "\n",
    "To avoid this problem, the *soft margin classification* is the one that is generallly used. The idea of the soft margin classification is to limit the amount of instaces that are in the wrong side of the street while keeping the street as wide as possible. The way to control this trade-off in Scikit-Learn is using the hyperparameter C. The algorithm sets a wider street with more margin violations when C is set in a small value, while there are less street violation but a narrower street when C is set high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2,3)]\n",
    "y = (iris['target'] == 2).astype(np.float64)\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "    ))\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of implemeting a Linear SVM classifier is using the `SVC()` class and setting `kernel='linear`. However, this option is much slower than `LinearSVC` when it comes to large datasets.\n",
    "\n",
    "Other option is to use the stochastic gradient descent classifier `SGDClassifier` setting `loss=hinge` and `alpha=1/(m*C)`. This method does not converge as fast as `LinearSVC` but is better at handling large datasets that do not fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

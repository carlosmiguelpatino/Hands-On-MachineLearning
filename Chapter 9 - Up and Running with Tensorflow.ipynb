{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Graph in a Session\n",
    "\n",
    "The best way to run a session is using a `with` statement. The `with` statement the current session is set as the default session. Also, it is preferred to initiliaze all global variables using the `global_variables_initializer()` function. This function does not automatically initializes the variables, but created a node that has to be ran with the session is opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Managing Graphs\n",
    "\n",
    "A node created during a session is automtically added to the default graph. Sometimes it is necessary to create multiple independent graphs, so a new graph has to be created a make it the default graph inside a `with` statement.\n",
    "\n",
    "It is common that while using the Jupyter Notebook or the Python shell some commands are run several times. That leads to having duplicate nodes in a graph. To reset the default graph, the command `tf.reset_default_graph()` can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lifecycle of a Node Value\n",
    "\n",
    "When evaluating a node $x$, TensorFlow first determines the nodes on which the node `x` depends. Then, the nodes on which the node `x` depends on are evaluated first. \n",
    "\n",
    "In the code below, the evaluation of node `y` makes TensorFlow evaluate nodes `w` and `x` first. The same process is carried on for node `z`. It has to be taken into account that `w` and `x` are being evaluated twice: one for `y` and one for `z`. \n",
    "\n",
    "```python\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())\n",
    "```\n",
    "\n",
    "To evaluate `x` and `w` efficiently, the code must ask TensorFlow to evaluate `y` and `z` in one graph.\n",
    "\n",
    "```python\n",
    "with tf.Session as sess:\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)\n",
    "```\n",
    "\n",
    "When single-process TensorFlow is used, each session has its copy of every variable. In distributed TensorFlow, the variable state is stored in the servers, so multiple sessions can share the same variables.\n",
    "\n",
    "# Linear Regression with TensorFlow\n",
    "\n",
    "The code below implements linear regression for the California housing dataset using TensorFlow. The first step is to add a bias term using numpy. Then, two TensorFlow constants nodes are created to hold the data and the targets. `theta` is calculated based on the normal equation $\\hat{\\theta} = \\left(\\mathbf{X}^{T}\\cdot\\mathbf{X}\\right)^{-1}\\cdot \\mathbf{X}^{T}\\cdot \\mathbf{y}$. All the matrix operations needed to compute `theta` are included in TensorFlow. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent\n",
    "\n",
    "## Manually Computing the Gradients\n",
    "\n",
    "The code below implements gradient descent manually using TensorFlow. The gradient descent algorithm is straightforward to follow, but there are a few TensorFlow functions to keep in mind:\n",
    "\n",
    "+ `random_uniform()` function creates a node in the graph that generates a tensor with random values within the range specified in the function.\n",
    "+ The `assign()` function creates a node that assigns a new value to a variable. In this particular case, the function updates the variable `theta` with the calculated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 6.74645\n",
      "Epoch 100 MSE 4.87894\n",
      "Epoch 200 MSE 4.84628\n",
      "Epoch 300 MSE 4.83791\n",
      "Epoch 400 MSE 4.83146\n",
      "Epoch 500 MSE 4.82628\n",
      "Epoch 600 MSE 4.8221\n",
      "Epoch 700 MSE 4.8187\n",
      "Epoch 800 MSE 4.81595\n",
      "Epoch 900 MSE 4.81371\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='prediction')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m*tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta-learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Using autodiff\n",
    "\n",
    "Calculating gradients manually for an arbitrary functions can lead to inefficient code or even to a very difficult task. This is why it is better to use TensorFlow's autodiff feature; autodiff can calculate gradients automatically and efficiently. \n",
    "\n",
    "To implement autodiff in gradient descent for linear regression it is enough to change the gradients definition for\n",
    "\n",
    "```python\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "```\n",
    "The `gradients()` function takes an operation (`mse`) operation and a list of variables (`[theta]`). The function creates a list of operations (one list per variable) to compute the gradients of the operation with regards to each variable. In this particular case, the `gradients` node will compute the gradient vector of MSE with regards to `theta`. \n",
    "\n",
    "## Using an Optimizer\n",
    "\n",
    "The process of finding the minimum value using gradient descent can be made even easier using optimizer provided by TensorFlow. For example, to optimize the gradient descent the lines `gradient = ` and `training_op = ` should be replaced for \n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```\n",
    "\n",
    "If the optmizer has to be changed for the problem, it is enough to change the optimizer line. For example, the momentum optimizer can be used instead of the gradient descent optimizer.\n",
    "\n",
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "The best way to implement mini-batch gradient descent is to use placeholder nodes. This type of node do not perform any computation, it only outputs the data that it is given at runtime. Placeholder nodes are used to pass training data to TensorFlow. \n",
    "\n",
    "When a placceholder is created, it is necessary to specify the tensor's datatype. Also, the shape of the tensor can be specified using the shape parameter. A tensor with an arbitrary number of dimensions can be specified using `None`. The example below creates a placeholder node `A` that will contain floats and that has to be three-dimensional. Then, the values for `A` are passed using `feed_dict` specifying the value of the three-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]]})\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement mini-batch gradient descent, the way `X` and `y` are specified has to be changed; both have to be converted to placeholder nodes. Also, a batch size has to be defined along with the total number of batches. Finally, the mini-batches have to be provided during the execution phase when evaluating each of the nodes that depend on `X` and `y`. \n",
    "\n",
    "```python\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape(None, 1), name='y')\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y_batch})\n",
    "    best_theta = theta.eval()\n",
    "```\n",
    "\n",
    "# Saving and Restoring Models\n",
    "\n",
    "Once a model is trained, the parameters should be saved on disk so they can be used later. It is also useful to save checkpoints during training so if the computer crashes while training the model, the training can continue from the last checkpoint instead of starting from scratch.\n",
    "\n",
    "Saving and restoring a model in TensorFlow is easy. It is enough to create a `Saver` node at the end of the construction phase, ie after all the variable nodes are created. Then, in the execution phase, the `save()` method of this node should be called passing the session and the path of the checkpoint file.\n",
    "\n",
    "```python\n",
    "...\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "...\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, 'tmp/my_model.ckpt')\n",
    "            \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = thete.eval()\n",
    "    save_path = save.save(sess, 'tmp/my_model_final.ckpt')\n",
    "```\n",
    "\n",
    "To restore a model saved in a checkpoint file the process is similar. A `Saver()` node has to be created at the end of the construction phase. Then, at the beginning of the execution phase, the method `restore()` is called instead of initializing the variables.\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'tmp/my_model.ckpt')\n",
    "    \n",
    "```\n",
    "\n",
    "The saver node allows to specify which variables to save and restore; all variables are saved and restored by default. For example, the following line is called to save only the theta variable under the name \"weights\".\n",
    "\n",
    "```python\n",
    "saver = tf.train.Saver({'weights':theta})\n",
    "```\n",
    "\n",
    "Graphs are also saved by default by the saver nodes in a secondary file with the extension .meta. This graph can be loaded using `tf.train.import_meta_graph()`. This function adds the graph in the file to the default graph and returns a `Saver` that can be used to restores the graph's state (the variable values).\n",
    "\n",
    "```python\n",
    "save = tf.train.import_meta_graph('tmp/my_model_final.ckpt.meta')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'tmp/my_model_final.ckpt')\n",
    "```\n",
    "Restoring the graph allows to restore completely a saved model: the graph structure and the variable variables. This can be done without having the code that built the model.\n",
    "\n",
    "# Visualizing the Graph and Traininng Curves Using TensorBoard\n",
    "\n",
    "TensorBoard is TensorFlow's tool to visualize progress during training. TensorBoard displays interactive training stats in a web browser. Also, TensorBoard allows to browse through a graph using its definition. Browsing through the graph is a way to identify errors, find bottlenecks, etc.\n",
    "\n",
    "The first step is to change the program that implements mini-batch gradient descent so it writes the graph definition and training stats, such as the training error MSE, to a log directory. It is important to use different directories for each time the program runs, because TensorBoard would merge the stats if they are on the same directory. The easier solution to create different directories for each run is to create a time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "logdir = '{}/run-{}/'.format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add two lines at the end of the construction phase. The first line creates a node that evaluates the MSE value and writes it to a binary string called `mse_summary` compatible with TensorBoard. The second line creates a `FileWriter` that will write summaries to logfiles in the log directory. The first parameter specified the path to the log directory, and the second parameter is the graph that is going to be visualized. `FileWriter` will create the log directory if it does not exist and write the graph definition to a binary log file called *events file*.\n",
    "\n",
    "```python\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph)\n",
    "```\n",
    "\n",
    "Once the previous lines are added, the execution phase has to be updated to evaluate the mse_summary node during training. It is important to avoid logging training stats in every step since that can significantly slow training. The evaluation of this node would output a summary that is then written by the `file_writer`. The `file_writer` must be closed at the end of the execution. \n",
    "\n",
    "When running this program, an events file is created containing the graph definition and the MSE values. To access the TensorBoard visualization, the `tensorboard` command must be ran in the directory containing the log files. Then, in a web browser the address *http://0.0.0.0:6006/*. In the event tabs the are all the values of the MSE, while on the graph tab the graph of the process should be shown. \n",
    "\n",
    "To make the visualization clearer, the nodes that many connections to other nodes are separated to the right area. Nodes can be moved back and forth between the main graph and the auxiliary area by right-clicking on the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:                                                        \n",
    "    sess.run(init)                                                                \n",
    "\n",
    "    for epoch in range(n_epochs):                                                 \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()       \n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Scopes\n",
    "\n",
    "When using TensorFlow with more complex models, the graph can become cluttered with nodes. Name scopes are created to group related nodes. For example, the `error` and `mse` operations can be grouped in a name scope called `loss`. when a node is created within a name scope, the name of the node is going to be preceded by the name scope.\n",
    "\n",
    "```python\n",
    "\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "```\n",
    "\n",
    "In the graph displayed by TensorBoard, the `mse` and `error` nodes appear inside the `loss` namespace that appears collapsed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = scaled_housing_data_plus_bias[indices] \n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:                                                        \n",
    "    sess.run(init)                                                                \n",
    "\n",
    "    for epoch in range(n_epochs):                                                 \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()       \n",
    "    \n",
    "file_writer.close()\n",
    "\n",
    "    \n",
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modularity\n",
    "\n",
    "Sometimes repetitive code has to be written to perform a task. For this cases, TensorFlow allows the creation of functions for repetitive tasks. The code below is a function to build rectified linear units (ReLU).\n",
    "\n",
    "ReLU\n",
    "\n",
    "$h_{\\mathbf{w}, b} = \\max(\\mathbf{X\\cdot w} + b , 0)$\n",
    "\n",
    "The function definition is the same as a function used in Python. However, the difference when TensorFlow is used lies in that nodes with the same name are created every time the function is called. In these cases, TensorFlow adds an underscore and an index to make each node unique.\n",
    "\n",
    "To make a repetitive task more clearer, a name scope can be created when the function is called. TensorFlow also adds a unique index to each name space, so each name space will have unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X,w), b, name='z')\n",
    "        return tf.maximum(z,0., name='relu')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing Variables\n",
    "\n",
    "TensorFlow has a particular option for sharing variables: the `get_variable()` function. The behavior of this function is controlled by the current `variable_scope()` environment. \n",
    "\n",
    "To reuse a variable the `reuse` attribute in the `variable_scope` has to be set to `True`. \n",
    "\n",
    "The code below calls five times the `relu` function. The code takes care that the `reuse` attribute is set to `False` in the first code and to `True` in subsequent calls. That way, the threshold is defined in the first call and the value is reused in the subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope('relu'):\n",
    "        threshold = tf.get_variable('threshold', shape=(),\n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X,w), b, name='z')\n",
    "        return tf.maximum(z,threshold, name='max')\n",
    "    \n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope('relu',reuse=(relu_index >=1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

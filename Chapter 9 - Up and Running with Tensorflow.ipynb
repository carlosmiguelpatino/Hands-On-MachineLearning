{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Graph in a Session\n",
    "\n",
    "The best way to run a session is using a `with` statement. The `with` statement the current session is set as the default session. Also, it is preferred to initiliaze all global variables using the `global_variables_initializer()` function. This function does not automatically initializes the variables, but created a node that has to be ran with the session is opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Managing Graphs\n",
    "\n",
    "A node created during a session is automtically added to the default graph. Sometimes it is necessary to create multiple independent graphs, so a new graph has to be created a make it the default graph inside a `with` statement.\n",
    "\n",
    "It is common that while using the Jupyter Notebook or the Python shell some commands are run several times. That leads to having duplicate nodes in a graph. To reset the default graph, the command `tf.reset_default_graph()` can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lifecycle of a Node Value\n",
    "\n",
    "When evaluating a node $x$, TensorFlow first determines the nodes on which the node `x` depends. Then, the nodes on which the node `x` depends on are evaluated first. \n",
    "\n",
    "In the code below, the evaluation of node `y` makes TensorFlow evaluate nodes `w` and `x` first. The same process is carried on for node `z`. It has to be taken into account that `w` and `x` are being evaluated twice: one for `y` and one for `z`. \n",
    "\n",
    "```python\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())\n",
    "```\n",
    "\n",
    "To evaluate `x` and `w` efficiently, the code must ask TensorFlow to evaluate `y` and `z` in one graph.\n",
    "\n",
    "```python\n",
    "with tf.Session as sess:\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)\n",
    "```\n",
    "\n",
    "When single-process TensorFlow is used, each session has its copy of every variable. In distributed TensorFlow, the variable state is stored in the servers, so multiple sessions can share the same variables.\n",
    "\n",
    "# Linear Regression with TensorFlow\n",
    "\n",
    "The code below implements linear regression for the California housing dataset using TensorFlow. The first step is to add a bias term using numpy. Then, two TensorFlow constants nodes are created to hold the data and the targets. `theta` is calculated based on the normal equation $\\hat{\\theta} = \\left(\\mathbf{X}^{T}\\cdot\\mathbf{X}\\right)^{-1}\\cdot \\mathbf{X}^{T}\\cdot \\mathbf{y}$. All the matrix operations needed to compute `theta` are included in TensorFlow. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent\n",
    "\n",
    "## Manually Computing the Gradients\n",
    "\n",
    "The code below implements gradient descent manually using TensorFlow. The gradient descent algorithm is straightforward to follow, but there are a few TensorFlow functions to keep in mind:\n",
    "\n",
    "+ `random_uniform()` function creates a node in the graph that generates a tensor with random values within the range specified in the function.\n",
    "+ The `assign()` function creates a node that assigns a new value to a variable. In this particular case, the function updates the variable `theta` with the calculated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 7.83235\n",
      "Epoch 100 MSE 5.10969\n",
      "Epoch 200 MSE 4.98783\n",
      "Epoch 300 MSE 4.93502\n",
      "Epoch 400 MSE 4.89822\n",
      "Epoch 500 MSE 4.87175\n",
      "Epoch 600 MSE 4.85266\n",
      "Epoch 700 MSE 4.83889\n",
      "Epoch 800 MSE 4.82896\n",
      "Epoch 900 MSE 4.8218\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='prediction')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m*tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta-learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Using autodiff\n",
    "\n",
    "Calculating gradients manually for an arbitrary functions can lead to inefficient code or even to a very difficult task. This is why it is better to use TensorFlow's autodiff feature; autodiff can calculate gradients automatically and efficiently. \n",
    "\n",
    "To implement autodiff in gradient descent for linear regression it is enough to change the gradients definition for\n",
    "\n",
    "```python\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "```\n",
    "The `gradients()` function takes an operation (`mse`) operation and a list of variables (`[theta]`). The function creates a list of operations (one list per variable) to compute the gradients of the operation with regards to each variable. In this particular case, the `gradients` node will compute the gradient vector of MSE with regards to `theta`. \n",
    "\n",
    "## Using an Optimizer\n",
    "\n",
    "The process of finding the minimum value using gradient descent can be made even easier using optimizer provided by TensorFlow. For example, to optimize the gradient descent the lines `gradient = ` and `training_op = ` should be replaced for \n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```\n",
    "\n",
    "If the optmizer has to be changed for the problem, it is enough to change the optimizer line. For example, the momentum optimizer can be used instead of the gradient descent optimizer.\n",
    "\n",
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "The best way to implement mini-batch gradient descent is to use placeholder nodes. This type of node do not perform any computation, it only outputs the data that it is given at runtime. Placeholder nodes are used to pass training data to TensorFlow. \n",
    "\n",
    "When a placceholder is created, it is necessary to specify the tensor's datatype. Also, the shape of the tensor can be specified using the shape parameter. A tensor with an arbitrary number of dimensions can be specified using `None`. The example below creates a placeholder node `A` that will contain floats and that has to be three-dimensional. Then, the values for `A` are passed using `feed_dict` specifying the value of the three-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]]})\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement mini-batch gradient descent, the way `X` and `y` are specified has to be changed; both have to be converted to placeholder nodes. Also, a batch size has to be defined along with the total number of batches. Finally, the mini-batches have to be provided during the execution phase when evaluating each of the nodes that depend on `X` and `y`. \n",
    "\n",
    "```python\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape(None, 1), name='y')\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    #load data from disk\n",
    "    return X_batch, y_batch\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y_batch})\n",
    "    best_theta = theta.eval()\n",
    "```\n",
    "\n",
    "# Saving and Restoring Models\n",
    "\n",
    "Once a model is trained, the parameters should be saved on disk so they can be used later. It is also useful to save checkpoints during training so if the computer crashes while training the model, the training can continue from the last checkpoint instead of starting from scratch.\n",
    "\n",
    "Saving and restoring a model in TensorFlow is easy. It is enough to create a `Saver` node at the end of the construction phase, ie after all the variable nodes are created. Then, in the execution phase, the `save()` method of this node should be called passing the session and the path of the checkpoint file.\n",
    "\n",
    "```python\n",
    "...\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "...\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, 'tmp/my_model.ckpt')\n",
    "            \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = thete.eval()\n",
    "    save_path = save.save(sess, 'tmp/my_model_final.ckpt')\n",
    "```\n",
    "\n",
    "To restore a model saved in a checkpoint file the process is similar. A `Saver()` node has to be created at the end of the construction phase. Then, at the beginning of the execution phase, the method `restore()` is called instead of initializing the variables.\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'tmp/my_model.ckpt')\n",
    "    \n",
    "```\n",
    "\n",
    "The saver node allows to specify which variables to save and restore; all variables are saved and restored by default. For example, the following line is called to save only the theta variable under the name \"weights\".\n",
    "\n",
    "```python\n",
    "saver = tf.train.Saver({'weights':theta})\n",
    "```\n",
    "\n",
    "Graphs are also saved by default by the saver nodes in a secondary file with the extension .meta. This graph can be loaded using `tf.train.import_meta_graph()`. This function adds the graph in the file to the default graph and returns a `Saver` that can be used to restores the graph's state (the variable values).\n",
    "\n",
    "```python\n",
    "save = tf.train.import_meta_graph('tmp/my_model_final.ckpt.meta')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'tmp/my_model_final.ckpt')\n",
    "```\n",
    "Restoring the graph allows to restore completely a saved model: the graph structure and the variable variables. This can be done without having the code that built the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

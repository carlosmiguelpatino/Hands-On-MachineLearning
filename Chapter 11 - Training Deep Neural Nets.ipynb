{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problems\n",
    "\n",
    "A problem deep neural networks have is that gradients tend to get smaller as Gradient Descent advances to lower layers. This is why Gradient Descent almost does not change the weights from lower layers preventing the training to converging to a good solution. This problem is called *vanishing gradients*. Sometimes the opposite scenario can occur: gradients can grow bigger, because many layers get large weights updates making the algorithm diverge. This is called the *exploding gradients* problem and is most commonly found in recurrent neural networks.\n",
    "\n",
    "This problems can be solved by using a specific initialization of the weights and changing the activation function of the neurons. These two changes prevented the network from increasing the variance after each level. This increment in the variance caused the activation function to saturate at the top layers. This saturation caused that during backpropagation the gradient was calculated in the saturated region of the activation function; this caused that the little gradient in the activation function was diluted when brackpropagation advanced to lower layers. \n",
    "\n",
    "## Xavier and He Initialization\n",
    "\n",
    "\n",
    "The Xavier and Xe initialization is based on the idea that signal has to flow properly both forward and backwards: forwards when making predictions and backwards when backpropagatin gradients. It is not possible to guarantee the proper behavior of the two directions unless the layer has the same number of inputs and outputs. However, it is has been proven that initializing the weights with certain distributions improves the performance.\n",
    "\n",
    "The first way to initialize the weights in a layer is using a normal distribution with **mean 0** and a standard deviation determined by the equation\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n_{inputs}+ n_{outputs}}}$$\n",
    "\n",
    "The second distribution used to initialize weights is a uniform distribution between $-r$ and $r$, with $r$ defined according to\n",
    "\n",
    "$$r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$$\n",
    "\n",
    "The equations for $\\sigma$ and $r$ shown above are used when the activation function is a logistic function. However, similar definitions are valid for activation functions such as Hyperbolic tangent and ReLU and its variations.\n",
    "\n",
    "The `tf.layers.dense()` function uses Xavier initialization (uniform distribution) by default. This initialization can be changed to the He initialization by using the following below. It has to be remembered that the He initialization only considers the number of inputs. To include both the number of inputs and outputs, the argument `mode='FAN_AVG'` has to be set in the `variance_scaling_initializer()` function.\n",
    "\n",
    "```python\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.contrib.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                           kernel_initializer=he_init, name='hidden1')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Nonsaturating Activation Functions\n",
    "\n",
    "One of the reasons for vanishing and exploding gradients problems was found to be the activation function choice. This is why now more functions besides the sigmoid activation function are used. \n",
    "\n",
    "**ReLU activation function:**\n",
    "\n",
    "One of the main advantages of the ReLU activation function is that it does not saturate for positive values. Also, it is a function that is is fast to calculate.\n",
    "\n",
    "**Leaky ReLU:**\n",
    "\n",
    "One of the problems with strict ReLU is that sometimes neurons die (keep ouputting zero) and cannot come back to life. A way to solve this problem is to define a leaky ReLU function. That is, a ReLU function that, instead of having a zero value for $z \\lt 0$, has a small loop defined by a hyperparameter $\\alpha$. This small slope allows the existence of a gradient for values negative values of $z$ so that way neurons can remain dormant for some time but come back to life at a certain point. It has been found out that leaky ReLUs outperform strict ReLUs and also prevent overfitting.\n",
    "\n",
    "**ELU Activation Function:**\n",
    "\n",
    "Other activation that is commonly used besided the ReLUs is the Exponential Linear Unit. This function is similar to a ReLU, but instead of a linear function has and exponential function for $z \\lt 0$. The ELU has the advantage of having an average output closer to zero because the exponential part takes negative values for $z \\lt 0$. This average helps alleviate problems related with vanishing gradients. Furthermore, ELU functions have a non-zero gradient for negative values of $z$, so problems related with dying neurons are avoided. Also, ELUs are smooth for all values of $z$, so Gradient Descent converges faster. Although ELUs are slower to compute that ReLUs, the computing time of training is compensated with the performance improvement of Gradient Descent. Nevertheless, ELUs will take longer than ReLUs in the test time. \n",
    "\n",
    "To use ELU activation functions in TensorFlow, it is enough to set the `activation` argument in the `dense()` function to `elu()` as \n",
    "\n",
    "```python\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name='hidden1')`\n",
    "```\n",
    "\n",
    "Altough TensorFlow does not have a function for leaky ReLUs, the following code is an implementation of this type of function.\n",
    "\n",
    "```python\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01*z, z, name=name)\n",
    "    \n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name='hidden1')\n",
    "```\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "Altough using the appropiate activation function and initializing the weights with the correct methods can solve vanishing/exploding gradients issues at the beginning of training, these problems can reappear during training. Batch Normalization addresses this issue by solving the problem of the change of input distribution in each layer when parameters of the previous layers change. The process described below consists in zero-centering and normalizing the inputs of the layer before the activation function is computed. The algorithm needs to calculate the mean and the standard deviation, so these quantities are evaluated for the inputs of the current mini-batch.\n",
    "\n",
    "1. $\\mu_{B} = \\frac{1}{m_{B}}\\sum^{m_{B}}_{i=1} \\mathbf{x}^{(i)}$\n",
    "\n",
    "2. $\\sigma_{B}^{2} = \\frac{1}{m_{B}} \\sum_{i=1}^{m_{B}} \\left(\\mathbf{x}^{(i)} - \\mu_{B} \\right)^{2} $\n",
    "\n",
    "3. $\\mathbf{\\hat{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}$\n",
    "\n",
    "4. $\\mathbf{z}^{(i)} = \\gamma \\mathbf{\\hat{x}}^{(i)} + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
